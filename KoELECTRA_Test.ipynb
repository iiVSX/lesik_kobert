{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoELECTRA_Test.ipynb","provenance":[{"file_id":"1GsDbxka0nsk8ybzfGo3WkgGkyLfhofzE","timestamp":1658803992993},{"file_id":"1mjk2Nzdm__GnSUcjiJCAGeEWVeS7B9tl","timestamp":1658414474037}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLPcesCOJwec","executionInfo":{"status":"ok","timestamp":1658807084746,"user_tz":-540,"elapsed":243173,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"66b29257-3d13-458b-f769-6227e812d24c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F87sG4MuIpeh","executionInfo":{"status":"ok","timestamp":1658807095780,"user_tz":-540,"elapsed":11053,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"415fbe74-cac5-4dd4-d6ff-108c257c3bdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 32.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 52.1 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm, tqdm_notebook\n","from keras import optimizers\n","from transformers import AdamW\n","from transformers import ElectraForTokenClassification, ElectraTokenizerFast\n","import torch\n","import numpy as np\n","from transformers import BertForTokenClassification\n","import pandas as pd"],"metadata":{"id":"oR5aSpR6KAhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/gdrive/MyDrive/2022_lesik_workspace/말뭉치/dataset.tsv', sep = '\\t', keep_default_na=False)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"fFaEPnf6K-Wf","executionInfo":{"status":"ok","timestamp":1658808214853,"user_tz":-540,"elapsed":1530,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"e055ff6e-f3fe-43c1-b8ee-de597cd923fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                text  \\\n","0                            [횡설수설/권순활]北 ‘외화벌이’ 뜯어먹기   \n","1  필리핀 국민의 약 10%인 800만 명은 세계 곳곳에서 건설노동자 가정부 유모 등으...   \n","2  본국의 가족에게 보내는 송금 총액은 매년 100억 달러를 넘어 필리핀 경제를 지탱하...   \n","3        멕시코 파키스탄 방글라데시 베트남 역시 해외파견 근로자의 송금이 한몫을 한다.   \n","4  한국도 과거 개발연대 시절 서독에 나간 광원과 간호사, 베트남과 중동에 진출한 근로...   \n","\n","                                               label  \n","0  O O O O O O PS_NAME PS_NAME O LCP_COUNTRY O O ...  \n","1  LCP_COUNTRY O O QT_PERCENTAGE QT_PERCENTAGE QT...  \n","2  O O CV_RELATION O O O O O O O O QT_PRICE QT_PR...  \n","3  LCP_COUNTRY LCP_COUNTRY LCP_COUNTRY LCP_COUNTR...  \n","4  LCP_COUNTRY O O O O O LCP_COUNTRY O O CV_OCCUP...  "],"text/html":["\n","  <div id=\"df-d7b97aa8-0373-41e5-bd6f-34ebe30c6a40\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[횡설수설/권순활]北 ‘외화벌이’ 뜯어먹기</td>\n","      <td>O O O O O O PS_NAME PS_NAME O LCP_COUNTRY O O ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>필리핀 국민의 약 10%인 800만 명은 세계 곳곳에서 건설노동자 가정부 유모 등으...</td>\n","      <td>LCP_COUNTRY O O QT_PERCENTAGE QT_PERCENTAGE QT...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>본국의 가족에게 보내는 송금 총액은 매년 100억 달러를 넘어 필리핀 경제를 지탱하...</td>\n","      <td>O O CV_RELATION O O O O O O O O QT_PRICE QT_PR...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>멕시코 파키스탄 방글라데시 베트남 역시 해외파견 근로자의 송금이 한몫을 한다.</td>\n","      <td>LCP_COUNTRY LCP_COUNTRY LCP_COUNTRY LCP_COUNTR...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>한국도 과거 개발연대 시절 서독에 나간 광원과 간호사, 베트남과 중동에 진출한 근로...</td>\n","      <td>LCP_COUNTRY O O O O O LCP_COUNTRY O O CV_OCCUP...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7b97aa8-0373-41e5-bd6f-34ebe30c6a40')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d7b97aa8-0373-41e5-bd6f-34ebe30c6a40 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d7b97aa8-0373-41e5-bd6f-34ebe30c6a40');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["데이터 레이블 리스트 형성"],"metadata":{"id":"yr5ayNN7Lsf_"}},{"cell_type":"code","source":["\n","\n","print(df['label'].values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_CDSb4AZ0C5","executionInfo":{"status":"ok","timestamp":1658807121299,"user_tz":-540,"elapsed":352,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"0dd39ae5-8851-451c-854c-ed51dc5fb237"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['O O O O O O PS_NAME PS_NAME O LCP_COUNTRY O O O O O O O'\n"," 'LCP_COUNTRY O O QT_PERCENTAGE QT_PERCENTAGE QT_PERCENTAGE O QT_MAN_COUNT QT_MAN_COUNT QT_MAN_COUNT O O O O O CV_OCCUPATION CV_OCCUPATION CV_OCCUPATION CV_OCCUPATION CV_OCCUPATION CV_OCCUPATION O O O O O O O'\n"," 'O O CV_RELATION O O O O O O O O QT_PRICE QT_PRICE QT_PRICE O O LCP_COUNTRY O O O O O CV_RELATION O O O O O O'\n"," ... 'O CV_OCCUPATION O O'\n"," 'O O OGG_POLITICS OGG_POLITICS OGG_POLITICS O O O O'\n"," 'DT_WEEK DT_WEEK O O O O O O']\n"]}]},{"cell_type":"code","source":["# Split labels based on whitespace and turn them into a list\n","unique_labels = set()\n","for label in df['label'].values.tolist():\n","  for i in label.split():\n","    if i not in unique_labels:\n","      unique_labels.add(i)\n","\n","\n","print(unique_labels)\n","\n","# Map each label into its id representation and vice versa\n","labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n","ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n","print(labels_to_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lx5HgHShLwS0","executionInfo":{"status":"ok","timestamp":1658808226919,"user_tz":-540,"elapsed":1034,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"65e9ad17-d72b-4a93-a688-f678f49edf13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'LCP_CITY', 'AFA_ART_CRAFT', 'QT_ALBUM', 'CV_POSITION', 'TM_SPORTS', 'PT_FLOWER', 'AF_MUSICAL_INSTRUMENT', 'AF_TRANSPORT', 'PS_NAME', 'EV_SPORTS', 'QT_PERCENTAGE', 'QT_SPEED', 'TR_HUMANITIES', 'QT_SIZE', 'CV_SPORTS', 'OGG_OTHERS', 'DT_YEAR', 'MT_METAL', 'OGG_SPORTS', 'OGG_LAW', 'CV_CULTURE', 'TI_MINUTE', 'DT_DURATION', 'TMIG_GENRE', 'AM_FISH', 'CV_SPORTS_POSITION', 'FD_SOCIAL_SCIENCE', 'PT_TYPE', 'QT_AGE', 'AM_TYPE', 'QT_OTHERS', 'CV_DRINK', 'TMI_PROJECT', 'OGG_MEDICINE', 'TMI_HW', 'CV_LAW', 'PT_OTHERS', 'DT_OTHERS', 'LCG_MOUNTAIN', 'OGG_EDUCATION', 'QT_WEIGHT', 'LCG_OCEAN', 'QT_MAN_COUNT', 'AM_MAMMALIA', 'QT_PHONE', 'OGG_MEDIA', 'CV_POLICY', 'PT_PART', 'CV_CLOTHING', 'OGG_MILITARY', 'OGG_RELIGION', 'LCG_RIVER', 'MT_ROCK', 'DT_MONTH', 'TI_SECOND', 'DT_WEEK', 'TI_DURATION', 'LCG_CONTINENT', 'OGG_SCIENCE', 'QT_VOLUME', 'MT_CHEMICAL', 'TM_COLOR', 'LCP_PROVINCE', 'CV_LANGUAGE', 'TI_HOUR', 'OGG_ECONOMY', 'TR_MEDICINE', 'TMI_MODEL', 'EV_OTHERS', 'EV_ACTIVITY', 'FD_SCIENCE', 'O', 'TR_SOCIAL_SCIENCE', 'AFA_VIDEO', 'CV_FUNDS', 'TM_DIRECTION', 'PT_FRUIT', 'CV_ART', 'DT_GEOAGE', 'CV_FOOD', 'TMM_DRUG', 'OGG_LIBRARY', 'FD_ART', 'OGG_FOOD', 'QT_ORDER', 'QT_SPORTS', 'AM_OTHERS', 'CV_TAX', 'TMI_SW', 'TR_OTHERS', 'LCG_BAY', 'AFA_PERFORMANCE', 'AF_BUILDING', 'AFA_MUSIC', 'OGG_ART', 'TM_CLIMATE', 'AFW_OTHER_PRODUCTS', 'AM_REPTILIA', 'FD_HUMANITIES', 'TMI_SERVICE', 'OGG_POLITICS', 'AF_WEAPON', 'AM_BIRD', 'LC_OTHERS', 'LCP_COUNTRY', 'QT_PRICE', 'LCP_CAPITALCITY', 'DT_DAY', 'AF_CULTURAL_ASSET', 'CV_CURRENCY', 'CV_SPORTS_INST', 'TR_ART', 'EV_FESTIVAL', 'PS_CHARACTER', 'CV_FOOD_STYLE', 'AM_PART', 'LCP_COUNTY', 'CV_RELATION', 'LCG_ISLAND', 'FD_OTHERS', 'DT_SEASON', 'AF_ROAD', 'QT_CHANNEL', 'QT_TEMPERATURE', 'QT_COUNT', 'PT_TREE', 'TMI_SITE', 'AM_INSECT', 'CV_PRIZE', 'CV_OCCUPATION', 'TI_OTHERS', 'OGG_HOTEL', 'LC_SPACE', 'TM_SHAPE', 'CV_BUILDING_TYPE', 'FD_MEDICINE', 'PT_GRASS', 'EV_WAR_REVOLUTION', 'PS_PET', 'TR_SCIENCE', 'TM_CELL_TISSUE_ORGAN', 'TMM_DISEASE', 'QT_ADDRESS', 'MT_ELEMENT', 'AFA_DOCUMENT', 'DT_DYNASTY', 'AFW_SERVICE_PRODUCTS', 'AM_AMPHIBIA', 'QT_LENGTH', 'CV_TRIBE'}\n","{'AFA_ART_CRAFT': 0, 'AFA_DOCUMENT': 1, 'AFA_MUSIC': 2, 'AFA_PERFORMANCE': 3, 'AFA_VIDEO': 4, 'AFW_OTHER_PRODUCTS': 5, 'AFW_SERVICE_PRODUCTS': 6, 'AF_BUILDING': 7, 'AF_CULTURAL_ASSET': 8, 'AF_MUSICAL_INSTRUMENT': 9, 'AF_ROAD': 10, 'AF_TRANSPORT': 11, 'AF_WEAPON': 12, 'AM_AMPHIBIA': 13, 'AM_BIRD': 14, 'AM_FISH': 15, 'AM_INSECT': 16, 'AM_MAMMALIA': 17, 'AM_OTHERS': 18, 'AM_PART': 19, 'AM_REPTILIA': 20, 'AM_TYPE': 21, 'CV_ART': 22, 'CV_BUILDING_TYPE': 23, 'CV_CLOTHING': 24, 'CV_CULTURE': 25, 'CV_CURRENCY': 26, 'CV_DRINK': 27, 'CV_FOOD': 28, 'CV_FOOD_STYLE': 29, 'CV_FUNDS': 30, 'CV_LANGUAGE': 31, 'CV_LAW': 32, 'CV_OCCUPATION': 33, 'CV_POLICY': 34, 'CV_POSITION': 35, 'CV_PRIZE': 36, 'CV_RELATION': 37, 'CV_SPORTS': 38, 'CV_SPORTS_INST': 39, 'CV_SPORTS_POSITION': 40, 'CV_TAX': 41, 'CV_TRIBE': 42, 'DT_DAY': 43, 'DT_DURATION': 44, 'DT_DYNASTY': 45, 'DT_GEOAGE': 46, 'DT_MONTH': 47, 'DT_OTHERS': 48, 'DT_SEASON': 49, 'DT_WEEK': 50, 'DT_YEAR': 51, 'EV_ACTIVITY': 52, 'EV_FESTIVAL': 53, 'EV_OTHERS': 54, 'EV_SPORTS': 55, 'EV_WAR_REVOLUTION': 56, 'FD_ART': 57, 'FD_HUMANITIES': 58, 'FD_MEDICINE': 59, 'FD_OTHERS': 60, 'FD_SCIENCE': 61, 'FD_SOCIAL_SCIENCE': 62, 'LCG_BAY': 63, 'LCG_CONTINENT': 64, 'LCG_ISLAND': 65, 'LCG_MOUNTAIN': 66, 'LCG_OCEAN': 67, 'LCG_RIVER': 68, 'LCP_CAPITALCITY': 69, 'LCP_CITY': 70, 'LCP_COUNTRY': 71, 'LCP_COUNTY': 72, 'LCP_PROVINCE': 73, 'LC_OTHERS': 74, 'LC_SPACE': 75, 'MT_CHEMICAL': 76, 'MT_ELEMENT': 77, 'MT_METAL': 78, 'MT_ROCK': 79, 'O': 80, 'OGG_ART': 81, 'OGG_ECONOMY': 82, 'OGG_EDUCATION': 83, 'OGG_FOOD': 84, 'OGG_HOTEL': 85, 'OGG_LAW': 86, 'OGG_LIBRARY': 87, 'OGG_MEDIA': 88, 'OGG_MEDICINE': 89, 'OGG_MILITARY': 90, 'OGG_OTHERS': 91, 'OGG_POLITICS': 92, 'OGG_RELIGION': 93, 'OGG_SCIENCE': 94, 'OGG_SPORTS': 95, 'PS_CHARACTER': 96, 'PS_NAME': 97, 'PS_PET': 98, 'PT_FLOWER': 99, 'PT_FRUIT': 100, 'PT_GRASS': 101, 'PT_OTHERS': 102, 'PT_PART': 103, 'PT_TREE': 104, 'PT_TYPE': 105, 'QT_ADDRESS': 106, 'QT_AGE': 107, 'QT_ALBUM': 108, 'QT_CHANNEL': 109, 'QT_COUNT': 110, 'QT_LENGTH': 111, 'QT_MAN_COUNT': 112, 'QT_ORDER': 113, 'QT_OTHERS': 114, 'QT_PERCENTAGE': 115, 'QT_PHONE': 116, 'QT_PRICE': 117, 'QT_SIZE': 118, 'QT_SPEED': 119, 'QT_SPORTS': 120, 'QT_TEMPERATURE': 121, 'QT_VOLUME': 122, 'QT_WEIGHT': 123, 'TI_DURATION': 124, 'TI_HOUR': 125, 'TI_MINUTE': 126, 'TI_OTHERS': 127, 'TI_SECOND': 128, 'TMIG_GENRE': 129, 'TMI_HW': 130, 'TMI_MODEL': 131, 'TMI_PROJECT': 132, 'TMI_SERVICE': 133, 'TMI_SITE': 134, 'TMI_SW': 135, 'TMM_DISEASE': 136, 'TMM_DRUG': 137, 'TM_CELL_TISSUE_ORGAN': 138, 'TM_CLIMATE': 139, 'TM_COLOR': 140, 'TM_DIRECTION': 141, 'TM_SHAPE': 142, 'TM_SPORTS': 143, 'TR_ART': 144, 'TR_HUMANITIES': 145, 'TR_MEDICINE': 146, 'TR_OTHERS': 147, 'TR_SCIENCE': 148, 'TR_SOCIAL_SCIENCE': 149}\n"]}]},{"cell_type":"markdown","source":["토큰화"],"metadata":{"id":"HTy4INQtL4_S"}},{"cell_type":"code","source":["# Let's take a look at how can we preprocess the text - Take first example\n","texts = df['text'].values.tolist()\n"],"metadata":{"id":"12PMY5-VL1Fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n","tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n","#text_tokenized = tokenizer(example, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n","\n","#print(text_tokenized)\n","#print(tokenizer.decode(text_tokenized.input_ids[0]))\n","#print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"],"metadata":{"id":"fOnAT57yL6lz","executionInfo":{"status":"ok","timestamp":1658808238693,"user_tz":-540,"elapsed":1755,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"03c08e87-ec3b-45a1-d7b1-e6ef6afc980e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at monologg/koelectra-base-v2-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v2-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["토큰화 후 레이블 조정"],"metadata":{"id":"g3fo5hOeMoHk"}},{"cell_type":"code","source":["'''\n","#print(text_tokenized)\n","word_ids = text_tokenized.word_ids()\n","print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n","print(word_ids)\n","print()\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"7ayfO-aEMh2s","executionInfo":{"status":"ok","timestamp":1658807154899,"user_tz":-540,"elapsed":278,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"e6721371-3484-4f1a-9865-469c433efdcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#print(text_tokenized)\\nword_ids = text_tokenized.word_ids()\\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\\nprint(word_ids)\\nprint()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["동일한 토큰에 속하는 모든 하위 단어 간에 동일한 레이블을 제공합니다. 없는 모든 토큰은 word_ids'-1'으로 레이블이 지정됩니다."],"metadata":{"id":"ChlMX9h8MxHf"}},{"cell_type":"code","source":["#label = labels[36]\n","\n","#If we set label_all_tokens to True.....\n","label_all_tokens = True\n","\n","#new_label = align_label_example(text_tokenized, label)\n","#print(new_label)\n","#print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"],"metadata":{"id":"hAfVpcmJM8uI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터세트 클래스"],"metadata":{"id":"bsuL8jyw7LfX"}},{"cell_type":"code","source":["def align_label(texts, labels):\n","    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-1)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]])\n","            except:\n","                label_ids.append(-1)\n","        else:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -1)\n","            except:\n","                label_ids.append(-1)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","        lb = [i.split() for i in df['label'].values.tolist()]\n","        txt = df['text'].values.tolist()\n","        self.texts = [tokenizer(str(i),\n","                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n","        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels"],"metadata":{"id":"Xy_adUBH7Oq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n","                            [int(.8 * len(df)), int(.9 * len(df))])\n","\n","print(df_train[0:10])"],"metadata":{"id":"9cIipoh07W_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658808266079,"user_tz":-540,"elapsed":556,"user":{"displayName":"오세훈","userId":"09984962564998164130"}},"outputId":"5415a496-a0bc-4193-f74c-c7d5621f358e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                     text  \\\n","140174                                  근데 밥은 한 끼도 안 먹었고?   \n","18801                           “철없던 시절에는 예솔이라는 이름이 싫었어요.   \n","42867                                장학금의 ‘나비효과’… 2년새 36억   \n","137916                       근데 이 굿와이프의 재밌는 점은 구성이 항상 다르고   \n","91340                 광복 뒤 맞이한 5월 첫째 일요일은 1946년 5월 5일이었다.   \n","143150                      어떤 사람이 옛한글자 팔백 몇 자를 만들 수 있대요.   \n","117851                      중국이 유럽연합(EU)과 ‘태양광 전쟁’에 돌입했다.   \n","59641   그러나 승객 91명이 골절과 찰과상 등 비교적 가벼운 상처를 입었다고 AFP통신이 ...   \n","25110   IS 공격을 피해 국경을 넘어 터키로 피신한 쿠르드족 난민도 18만6000여 명에 ...   \n","166771            불러서 이제 가장 먹고싶은 게 그 한국음식 된장찌개 뭐 이런 거드라구요   \n","\n","                                                    label  \n","140174          O CV_FOOD O QT_COUNT QT_COUNT O O O O O O  \n","18801   O O O O O O O PS_NAME PS_NAME PS_NAME O O O O ...  \n","42867   O O O TR_SOCIAL_SCIENCE TR_SOCIAL_SCIENCE O DT...  \n","137916  O O AFA_VIDEO AFA_VIDEO AFA_VIDEO O O O O O O ...  \n","91340   O O O O DT_MONTH DT_MONTH QT_ORDER DT_DAY O DT...  \n","143150  O O O CV_LANGUAGE CV_LANGUAGE CV_LANGUAGE O QT...  \n","117851  OGG_POLITICS O OGG_OTHERS O OGG_OTHERS O O O O...  \n","59641   O O QT_MAN_COUNT QT_MAN_COUNT O TMM_DISEASE O ...  \n","25110   OGG_MILITARY O O O O O O LCP_COUNTRY O O O CV_...  \n","166771  O O O O O O O O O O LCP_COUNTRY O O CV_FOOD CV...  \n"]}]},{"cell_type":"markdown","source":["모델 빌딩"],"metadata":{"id":"KQq6sxlw7Ul1"}},{"cell_type":"code","source":["class ElectraModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(ElectraModel, self).__init__()\n","        self.electra = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-base-v2-discriminator\", num_labels=len(unique_labels))\n","\n","    def forward(self, input_id, mask, label):\n","\n","        output = self.electra(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","\n","        return output"],"metadata":{"id":"8Cv53qH97a8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["훈련 루프"],"metadata":{"id":"iq8vb2Ea7d5Z"}},{"cell_type":"code","source":["def train_loop(model, df_train, df_val):\n","\n","    train_dataset = DataSequence(df_train)\n","    val_dataset = DataSequence(df_val)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    #device = torch.device(\"cuda:0\")\n","\n","    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    best_acc = 0\n","    best_loss = 1000\n","\n","    for epoch_num in range(EPOCHS):\n","\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","\n","        for train_data, train_label in tqdm(train_dataloader):\n","\n","            train_label = train_label[0].to(device)\n","            mask = train_data['attention_mask'][0].to(device)\n","            input_id = train_data['input_ids'][0].to(device)\n","\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","            logits_clean = logits[0][train_label != -100]\n","            label_clean = train_label[train_label != -100]\n","\n","            predictions = logits_clean.argmax(dim=1)\n","\n","            acc = (predictions == label_clean).float().mean()\n","            total_acc_train += acc\n","            total_loss_train += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label[0].to(device)\n","            mask = val_data['attention_mask'][0].to(device)\n","\n","            input_id = val_data['input_ids'][0].to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            logits_clean = logits[0][val_label != -100]\n","            label_clean = val_label[val_label != -100]\n","\n","            predictions = logits_clean.argmax(dim=1)          \n","\n","            acc = (predictions == label_clean).float().mean()\n","            total_acc_val += acc\n","            total_loss_val += loss.item()\n","\n","        val_accuracy = total_acc_val / len(df_val)\n","        val_loss = total_loss_val / len(df_val)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n","\n","LEARNING_RATE = 1e-2\n","EPOCHS = 5\n","\n","model = ElectraModel()\n","train_loop(model, df_train, df_val)"],"metadata":{"id":"knuKPJOiHbiZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51d2bbdc-c4ee-49dc-8243-b9b474a42560"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at monologg/koelectra-base-v2-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v2-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["path = \"/content/gdrive/MyDrive/2022_lesik_workspace/model/KoELECTRA.pt\"\n","# 저장하기\n","torch.save(model.state_dict(), path)"],"metadata":{"id":"cJbOtD9fYnN1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["테스트 데이터에 대한 모델 평가"],"metadata":{"id":"icTiLTi4HdFr"}},{"cell_type":"code","source":["def evaluate(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    total_acc_test = 0.0\n","\n","    for test_data, test_label in test_dataloader:\n","\n","        test_label = test_label[0].to(device)\n","        mask = test_data['attention_mask'][0].to(device)\n","        input_id = test_data['input_ids'][0].to(device)\n","          \n","        loss, logits = model(input_id, mask, test_label.long())\n","\n","        logits_clean = logits[0][test_label != -1]\n","        label_clean = test_label[test_label != -1]\n","\n","        predictions = logits_clean.argmax(dim=1)\n","              \n","        acc = (predictions == label_clean).float().mean()\n","        total_acc_test += acc\n","\n","    val_accuracy = total_acc_test / len(df_test)\n","    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","\n","\n","evaluate(model, df_test)"],"metadata":{"id":"oJKHuHA7Hekf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def align_word_ids(texts):\n","  \n","    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-1)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(1)\n","            except:\n","                label_ids.append(-1)\n","        else:\n","            try:\n","                label_ids.append(1 if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","\n","def evaluate_one_text(model, sentence):\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","\n","    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n","\n","    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n","\n","    logits = model(input_id, mask, None)\n","    logits_clean = logits[0][label_ids != -1]\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    print(sentence)\n","    print(prediction_label)\n"],"metadata":{"id":"Md-IS5gkHkZY","executionInfo":{"status":"ok","timestamp":1658472520445,"user_tz":-540,"elapsed":484,"user":{"displayName":"박지연","userId":"00986758167926625550"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5b069e9-e083-4880-ce5a-e5e27c8f94f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["파를 송송 썰고, 양파를 넣어주세요.\n","['B-eve', 'B-art', 'B-art', 'B-art', 'B-art', 'B-eve', 'B-art', 'B-eve', 'B-art', 'B-art', 'B-art', 'B-art', 'B-art', 'B-eve']\n"]}]},{"cell_type":"code","source":["model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n","tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v2-discriminator\")\n","word_ids = text_tokenized.word_ids()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TyTfagsarasE","executionInfo":{"status":"ok","timestamp":1658472746059,"user_tz":-540,"elapsed":3896,"user":{"displayName":"박지연","userId":"00986758167926625550"}},"outputId":"8b5f0035-038b-4f6b-f7de-26bf9d930bc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at monologg/koelectra-base-v2-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v2-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["example = '2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다'\n","text_tokenized = tokenizer(example, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n","evaluate_one_text(model, example)\n","print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n","print(word_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Smt9dDlXq9-c","executionInfo":{"status":"ok","timestamp":1658472794004,"user_tz":-540,"elapsed":11,"user":{"displayName":"박지연","userId":"00986758167926625550"}},"outputId":"1de673b1-3743-485d-f0eb-b5bfcce42ddb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다\n","['B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve', 'B-eve']\n","['[CLS]', '2009', '##년', '7', '##월', 'FC', '##서울', '##을', '떠나', '잉글랜드', '프리미어리그', '볼턴', '원더', '##러스', '##로', '이적', '##한', '이청용', '##은', '크리스탈', '팰리스', '##와', '독일', '분데스리가', '##2', 'V', '##f', '##L', '보', '##훔', '##을', '거쳐', '지난', '3', '##월', 'K', '##리그', '##로', '컴백', '##했', '##다', '.', '행', '##선지', '##는', '서울', '##이', '아닌', '울산', '##이', '##었', '##다', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","[None, 0, 0, 0, 1, 2, 2, 3, 3, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"]}]}]}